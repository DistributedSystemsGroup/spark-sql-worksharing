In Spark SQL, for each individual application, it is possible to indicate which data (DataFrame) to cache in memory. Subsequent data transformation can reuse cached data without the need to recompute it from the lineage. However, Spark applications bstantialsubmitted to a cluster by different clients are totally isolated from each other\cite{zaharia2012resilient}, therefore, sharing across applications is not trivial. To avoid modifying spark's internal architecture principle, we build a centralized Spark SQL server serving requests from multiple users while acting as a single client of cluster. The optimization we proposed is being done at this central Spark SQL server.

In our system, we use the optimized logical plans as the input for the query optimizer. Users have many interfaces to express the data they want in Spark SQL. Logical plan is the one that abstract and unified. The optimized logical plan is the last logical representation of a query before converting it to a physical. If the input queries are similar, looking at the optimized logical plans is the best way to discover the similar parts.

We follow the optimization process of 4 phases discussed in section \ref{sec:caching}. Operator fingerprints are computed to identify all similar subexpressions in the first phase. Phase 2 and phase 4 require some query transformations. We take advantage of the pattern matching feature of scala and the TreeNode library of Catalyst in Spark SQL to rewrite queries. Transforming rules are passed as a function to specify how to transform the trees. Pre-computed statistics data computed in phase 3 are done by launching 2 jobs for each table. The cardinality of each column is estimated using the algorithm HyperLogLog.