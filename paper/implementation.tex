%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Apache Spark and Spark SQL overview}
% \label{sec:}
% \input{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mytodo{Intro to spark, refer to spark paper: the paradigm, the abstraction, speed, ...}.
In this paper, we adopt an open-source processing engine: Apache Spark. Among the prevalent engines, Spark is proven to bring strong performance due to its in-memory computation nature. It is the first cluster computing engine that leverages the distributed memory abstraction and outperforms Hadoop by up to 20X in iterative applications \cite{zaharia2012resilient}. This motivated us to propose a new in-memory cache-based optimization for cloud environment. Spark SQL is the high level language support running on top of Spark. We present the implementation of our solution in the query optimizer of Spark SQL.

Spark SQL defines an SQL-like high-level language for processing large-scale analytic workloads. The queries expressed in the high-level languages are finally transformed into RDDs as the abstract concept of Spark core engine. Queries written in Spark SQL are in an abstract form: the DataFrames. A DataFrame object represents a \emph{logical plan} to compute a dataset. A logical plan is a tree composed of operators (nodes). Each has operator name (operator type) and attributes (filtering predicates, join columns, etc.). A node could be binary, unary or leaf node that has 2 children, 1 child or zero child respectively.

Catalyst \cite{sparksql} is the query optimizer of Spark SQL. Catalyst focuses on optimizing each single query, while in our approach, a cost-based optimizer is applied to optimize multiple queries together. The rule-based optimizer in Spark SQL defines the rules (constant folding, early filtering, and predicate pushdown, etc.) to produce optimized logical plans. Using the cost model to decide the joining strategies, the optimized logical plans are then transformed to single physical plan for execution. Figure \ref{fig:sparksql_queryplanning} illustrates the internal Spark SQL's architecture, and the modules involved in parsing and generating the various query plans discussed above.

\begin{figure*}[t]
   \centering
   \includegraphics[scale=0.45]{figures/sparksql_queryplanning}
   \caption{Phases of query planning in Spark SQL. Rounded rectangles represent Catalyst trees \cite{sparksql}} 
   \label{fig:sparksql_queryplanning}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation of work sharing for Spark SQL}
\label{sec:plan_implementation}
\input{plan_implementation}