Nowadays, more and more e-commerce companies have been embracing new technologies to cope with the huge expansion of their data. Big data analytics helps ones gain insights into the market as well as drive to rational business decisions. General-purpose cluster processing frameworks such as MapReduce \cite{dean2008mapreduce}, Dryad \cite{isard2007dryad} and Spark \cite{zaharia2012resilient} were developed to address those needs. Moreover, built on top of those, high-level SQL-like languages (Pig, DryadLINQ, Spark SQL respectively) have been gaining popularity, due to their simple and declarative nature together with rich automatic optimization.

In shared large-scale distributed systems, analytics jobs submitted by various users often share similar work, for example scanning and processing the same set of data. However, most prevalent query optimizers focus on finding an optimal plan for each job. Instead of optimizing them independently, which results in redundant and wasteful processing, many well-known multi-query optimization techniques could be employed to save a considerable amount of cluster resources. One typical instance is the research in \cite{gunda2010nectar} on 25 production clusters, estimated that over 35,000 hours of redundant computation can be eliminated per day by caching intermediate results (approximately equivalent to shutting off 1500 machines daily).

Some high-level languages support multiple query optimization, for example Pig and Spark SQL. However this feature is not achieved automatically and only available for single user. In Pig, users manually specify which data should be shared by using the ``Split" operator. In Spark SQL, users can explicitly invoke ``Cache" or ``Persist" operation on a DataFrame (a query) to keep the result in memory for later access. In fact, deciding by hand the best data to cache (materialize to RAM) might be a problematic task that is usually done by data experts. A better optimizer should use its knowledge to solve this task efficiently.

Established automatic solutions, for example the studies in similar subexpressions sharing \cite{zhou2007efficient} and materialized views selection \cite{goldstein2001optimizing, mistry2001materialized}, were proposed for RDBMSes. The idea of reusing intermediate data across queries or jobs in distributed environment has also received a significant body of research  recently: for MapReduce \cite{mrshare, mqo}, for SCOPE operating on top of Cosmos \cite{silva2012exploiting} and for Massive Parallel Processing (MPP) framework \cite{el2015optimization}. We will discuss these state-of-the-art studies in more detail in Section \ref{sec:related}. Applying multi-query optimization ideas into contemporary large-scale data analytic framework raises new challenges due to its scalability and distributed nature.

In this paper, we devise a different principle to achieve work sharing - in-memory cache primitives. By exploiting the shareable common or similar subexpressions and leveraging the full potential power of the distributed memory cache, our system - Spark SQL Server transforms a batch of SQL-like input queries into a new more efficient one which avoids unnecessary computations and reduces query processing cost. Notably, caching technique has recently gained attention in the research community, and many systems now implement some form of memory-backed storage \cite{tachyon, hdfs}.

Our optimization strategy begins with identifying the sharing opportunities: detect and exploit Similar subExpressions (SEs). SEs are identical or similar expressions that appear more than one in a query or in multiple queries submitted as batch. For example the common table expressions, which are defined once and referred multiple times in a query, are also SEs. Detected SEs should be executed only once by rewriting some queries to use Covering subExpression(s) (CEs). A CE, being constructed from a group of SEs, produces all common sharing tuples for the queries using it. We refer those queries as the \emph{consumers} of that CE. A CE will then become a \emph{cache plan} if it is chosen to be included in the final execution plan. The system ensures that \emph{cache plans} are computed only once and materialized in RAM so that later jobs can benefit from them. Due to the limited availability of cluster's memory, our objective of \emph{cache plans} selection is to minimize the total query processing under a given memory constraint. We modeled this dilemma as the Multiple-choice Knapsack problem and solved it using some heuristics and cost estimation. To the best of our knowledge, our approach is the first that could exploit the in-memory distributed caching to achieve high performance work sharing.

We summarize our main contributions:
\begin{enumerate}
	\item We believe that ours is the first paper that systematically explores how to apply multi-query optimization techniques along with the distributed in-memory cache to achieve high performance work sharing in distributed environment.
	\item We solve the optimization problem by building an optimizer as an extension for Spark SQL operating on top of the Spark engine. The optimizer avoids generating obviously bad plans as early as possible.
	\item We propose a general and abstract cost model, based on pre-computed statistics information, capable of estimating the efficiency of different plans and produce a globally near optimal execution plan.
	\item We extensively experiment on our prototype and demonstrate interesting characteristics and prospect for a high performance work sharing system for large-scale distributed environment.
\end{enumerate}

The rest of the paper is structured as follows:  Section \ref{sec:related} covers related work on multiple query optimization in traditional database management systems and cluster computing engines. We then introduce and formalize our optimization problem in Section \ref{sec:problem}. The overview on the idea of using cache primitives to achieve high performance for distributed system is discussed in Section \ref{sec:caching}. In Section \ref{sec:implementation}, we provide the implementation details of our idea for Apache Spark and Spark SQL. We evaluate the performance of our proposed approach in Section \ref{sec:evaluation}. Finally, we conclude the paper in Section \ref{sec:conclusion}.