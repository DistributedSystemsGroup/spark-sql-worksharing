%Khoa
We now present experimental results to evaluate the effectiveness of our prototype on Spark and Spark SQL. First, we focus on a detailed analysis of caching efficiency with respect to simple queries, that just focus on individual operators. We want to convey the sense of the effectiveness of data caching in work sharing system to the readers. We then proceed with a general overview of the performance gains achieved with our optimizer, using the standard TPC-DS benchmarking. The efficiency of our cost-model is also evaluated experimentally.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental setup}
\label{sec:setup}
%\include{setup}
We run the micro-benchmark on cluster consisting of 14 worker nodes. Each node has 6 GB RAM of which 3 GB is used for caching data. The data set is stored in HDFS (replication = 3) in Parquet and CSV format. 

For the micro-benchmark, the data sets are randomly generated. The schema of data set is as follows: \texttt{(Name: str[20], Age: int, Dep: int, Desc1: str[100], Desc2: str[200])}. The Age column follows uniform distribution in the range [1, 100] so that we can later adjust the input/output ratio. The data sets have various number of records and sizes: 10M records (3 GB), 30M (9 GB), 50M (15 GB), 80M (24 GB), 100M (30.6 GB), 130M (40 GB), 150M (46 GB) and 200M (61 GB).

For the macro-benchmark, we use TPC-DS benchmark library developed by Databricks for Spark and Spark SQL \cite{sparksqlperf}. The scaling factor is set to 10, 30, 50 and 100.

Before running each test, we clear the operating system's buffer cache to obtain more accurate result. Each single test is run 3 times and we take the average.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Micro-benchmarks}
\label{sec:micro}
In this Section, we evaluate our system through a series of experiments on simple queries. We focus on just simple ones to emphasize the benefits of caching as a function of the particular operators involved in each query. For simplicity, we consider workloads composed by two queries only, and compute the running time of the Spark jobs associated to each query, with and without our cache-based multi-query optimization mechanism. Overall, our results indicate:

\begin{itemize}
	\item The format of input files matters the increase of job's performance.
	\item Roughly 50\% and 30\% improvement in aggregate query latencies for CSV and Parquet files respectively.
	\item Memory utilization is minimized.
\end{itemize}

We use the following notation:
\begin{itemize}
	\item R\_Job1, R\_Job2: job execution times, with no caching
	\item R\_Job1*, R\_Job2*: job execution times, with caching
	\item M\_R: Total memory utilization, in Bytes
	\item M\_D: Total disk utilization, in Bytes. This metric accounts for data that does not fit in RAM, and is spilled to disk by the Spark caching mechanism.
	\item M: total memory caching capacity
	\item I: input size of the dataset, in terms of records.
\end{itemize}

We first examine Filter and Project operations because they appear very frequently in data analysis, and are usually pushed as close as possible to the input by traditional query optimization techniques. Then, we consider the join operation that involves shuffling data across the network.

\subsection{Filter-based queries}
The two queries are described by the logical plans depicted in Figure \ref{fig:query1_plans}. The top left and top right logical plans indicate that the two queries we consider read data from the same input relation, then apply a filter operation with two different predicates. The logical plan displayed at the bottom of the Figure is the result output by our multi-query optimizer: Job1* first reads the data from the input relation, then applies a Filter operator, with a combined predicate. Next, Job1* caches the filtered data and finally applies its own filter predicate to produce the output. The second job Job2*, reads directly its input data from the cache.

\begin{figure}[htbp]
   \centering
   \includegraphics[scale=0.5]{figures/query1_cacheplan}
   \caption{Query and Cache plan for Filter-based queries.} 
   \label{fig:query1_plans}
\end{figure}

Next, in Figure \ref{fig:query1}, we show the individual and aggregate query latencies, with and without our optimization. We also show the impact of the input data format, considering both Parquet and CSV input files.

\begin{figure}[!htb]
	\centering
	\subfigure[Parquet]{
   \includegraphics[scale=0.35]{figures/query1_parquet}
   \label{fig:query1_parquet}}

  	\subfigure[CSV]{
   \includegraphics[scale=0.35]{figures/query1_csv}
   \label{fig:query1_csv}}

   \caption{Filter-based queries, query latencies and memory utilization.}
   \label{fig:query1}
\end{figure}

The first set of results indicate the multi-query optimizer works as expected: for both Parquet and CSV input format, the aggregate query latency for the two jobs is shorter with caching optimization. In details, we notice that Job1* runs slower than Job1: this is attributed to the cost of the caching operation. However, the difference in runtime between Job1* and Job1 is not the same for the case Parquet (noticeable different) and CSV (somehow equals). Additionally, the execution time of Job2* (which involves reading data from cache then writing data to disk) for both formats shows different pattern: when data is in Parquet format, Job2* benefits less from caching.

The cache size is roughly 25\% of the size of input data, for both the Parquet and CSV case. Our multi-query optimizer uses only 1/3 of the caching capacity (25\% of 200M records costs 16GB of cache size, which totals 45GB in our system).

Next, we consider a different set of filtering conditions, which are more stressful with respect to memory utilization. The queries we consider are less selective than the ones presented above, with filtering predicates as follows: \texttt{age <= 50} and \texttt{age <= 25}. Figure \ref{fig:query1_50} shows query latencies and memory utilization respectively.

\begin{figure}[!htb]
	\centering

	\subfigure[Parquet]{
   \includegraphics[scale=0.35]{figures/query1_parquet_50}
   \label{fig:query1_parquet_50}}

  	\subfigure[CSV]{
   \includegraphics[scale=0.35]{figures/query1_csv_50}
   \label{fig:query1_csv_50}}

   \caption{Filter-based queries, query latencies and memory utilization.}
   \label{fig:query1_50}
\end{figure}

Finally, we consider even less selective predicates, defined as follows: (\texttt{age <= 80} and \texttt{age <= 15}). Figures \ref{fig:query1_80_15} show query latencies and memory utilization. For this set of filtering conditions the optimizer caches a lot of data in Job1, of which very little is useful for Job2: as a consequence, the cost of caching might not be balanced by its benefits. This shows that the amount of performance gain also depends on the cardinalities of expressions.

\begin{figure}[!htb]
	\centering

	\subfigure[Parquet]{
   \includegraphics[scale=0.35]{figures/query1_parquet_80_15}
   \label{fig:query1_parquet_80_15}}

  	\subfigure[CSV]{
   \includegraphics[scale=0.35]{figures/query1_csv_80_15}
   \label{fig:query1_csv_80_15}}

   \caption{Filter-based queries, query latencies and memory utilization.}
   \label{fig:query1_80_15}
\end{figure}

\subsection{Projection-based queries}
Next, we consider simple queries that only perform project operations. Specifically, with reference to the schema of our input data, queries project two attributes: \texttt{desc1:string[100]} and \texttt{desc2:string[200]}.

The original logical plans of the two queries, along with the optimized plan that uses caching are depicted in Figure \ref{fig:query2_plans}.

\begin{figure}[!htb]
   \centering
   \includegraphics[scale=0.5]{figures/query2_cacheplan}
   \caption{Query and Cache plans for Project-based queries.} 
   \label{fig:query2_plans}
\end{figure}

Next, we present the analysis of the query latencies with and without our optimization, for both Parquet and CSV input data types, in Figures \ref{fig:query2}.

\begin{figure}[!htb]
	\centering

	\subfigure[Parquet]{
   \includegraphics[scale=0.35]{figures/query2_parquet}
   \label{fig:query2_parquet}}

  	\subfigure[CSV]{
   \includegraphics[scale=0.35]{figures/query2_csv}
   \label{fig:query2_csv}}

   \caption{Project-based queries, query latencies and memory utilization.}
   \label{fig:query2}
\end{figure}

Our observations on the results for Project-based queries are as follows. 
When using CSV input data, the benefit of our optimization is clear. Instead, when using the Parquet format, our results indicate little to no benefit in using our optimization. Indeed, Parquet is geared towards columnar data access on disk: as a consequence, the cost associated to cache input data outweighs the benefits of reading two columns from RAM instead of using their disk representations.

\subsection{Projection and Filter based queries}
We now study the impact of our optimization on two queries that mix projection and filters. The logical plans and the optimized plan output by our approach are depicted in Figure \ref{fig:query3_plans}

\begin{figure}[!htb]
   \centering
   \includegraphics[scale=0.5]{figures/query3_cacheplan}
   \caption{Query and Cache plans for Project and filter based queries.} 
   \label{fig:query3_plans}
\end{figure}

Next, we present the analysis of the query latencies with and without our optimization, for both Parquet and CSV input data types, in Figures \ref{fig:query3}.

\begin{figure}[!htb]
	\centering

	\subfigure[Parquet]{
   \includegraphics[scale=0.35]{figures/query3_parquet}
   \label{fig:query3_parquet}}

  	\subfigure[CSV]{
   \includegraphics[scale=0.35]{figures/query3_csv}
   \label{fig:query3_csv}}

   \caption{Project-based queries, query latencies and memory utilization.}
   \label{fig:query3}
\end{figure}

Our results indicate that the ``economy'' of our optimization requires a detailed cost-based analysis: in particular, projections on parquet formatted input files do not benefit from caching, and the cost of the cache operation is larger than its benefits.

\subsection{Joining queries}
We now study consider queries that involve costly processing. In particular, ranking data requires ``shuffling'' data over the network and sorting.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Macro-benchmarks}
\label{sec:macro}
We also run the experiment on the TPC-DS queries that are compatible with Spark SQL, developed by the author of Spark and Spark SQL \cite{sparksqlperf}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cost model evaluation}
\label{sec:cost}