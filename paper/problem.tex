We discuss the optimization opportunities and challenges through the following example. We also provide the readers a more practical example of queries taken from the TPC-DS benchmark \cite{tpcds} in Section \ref{sec:tpcds-sharing}.

\begingroup
\fontsize{7pt}{8pt}
\selectfont
\begin{verbatim}
QUERY 1:
SELECT name, dept_name, salary
FROM employees, departments, salaries
WHERE dep = dept_id
	AND id = emp_id
	AND gender = 'F'
	AND location = 'us'
	AND salary > 20000
ORDER BY salary DESC

QUERY 2:
SELECT name, dept_name, title, 
	to as title_expired_on
FROM departments, employees, titles
WHERE dep = dept_id
	AND id = emp_id
	AND gender = 'F'
	AND location = 'us'
	AND from >= 2010

QUERY 3:
SELECT id, name, salary, from_date
FROM employees, salaries
WHERE id = emp_id
	AND age > 30
	AND SALARY > 30000
\end{verbatim}
\endgroup

\begin{figure*}[htbp]
	\centering
	\includegraphics[scale=0.65]{figures/common_sub}
	\caption{Optimized operator trees} 
	\label{fig:common_sub}
\end{figure*}

Looking at the \emph{FROM} statements above, we can see that 3 queries (jobs) can share the common reading of \textbf{employees}, \textbf{departments} and \textbf{salaries} tables. Although disk speed has increased recently, a possible optimization would be to avoid wasteful disk I/O. Indeed, I/O operations are heavyweight because they not only involve reading records from file, but also parsing and transforming those into objects in memory for processing. What if we could tell the first job to \emph{cache} the input data after reading it, so that the later jobs do not have to ``waste time'' doing redundant I/O operations? A more refined approach could be to find common work (similar subexpressions) among these queries (for instance reading input, filtering and projecting records, joining tables, etc.) so that we can assure we are ``maximizing'' sharing benefit while ``minimizing'' the cache size (memory utilization).

To facilitate reader's understanding, we use Figure \ref{fig:common_sub} which illustrates the optimized operator trees (optimized logical plans) of 3 queries in the above example. Those optimized logical plans are produced by Spark SQL's query optimizer that will be next translated into physical plans for execution. The leaf nodes represents the base relations. Each intermediate node is a relational algebra operator (Filter, Project, Join, etc.). The arrows indicate data flow.

As can be seen from Figure \ref{fig:common_sub}, multiple SEs can be identified. They have the same tree structure but different filtering predicates and projections, for example the similar subexpression 2 (SE2): $Project_p(Filter_f(employees))$ which appears in both 3 queries. We can save some reading, parsing, filtering and projecting costs on the \textbf{employees} table 2 times out of 3. In order to achieve that, a CE will be constructed by ``combining'' operators' predicates: 
\[\Scale[0.95]{Project_{id, name, dep, age, gender}(Filter_{gender=F \cup age>30}(employees))}\]

Constructing the CE in this way makes its output can be used to answer the consumer queries. By executing this CE and keeping the result in memory, the consumers can retrieve it back instead of reading and computing again from scratch. Thereby, the query execution time reduces significantly. Imagine that the Projection operator only projects on 5 out of total 10 columns and the Filtering keeps 60\% of records in \textbf{employees} tables, then the consumers rather than re-read and scan the whole table from disk, they can just extract from a smaller chunk of cached data in-memory. 

In the same way, SE3 and SE4 share the projection and filtering on \textbf{deparment} and  \textbf{salaries} tables respectively. Note that sharing some SEs is always better than some others, for instance the SE $Project_p(Filter_f(employees))$ does more computations but never produces more output data than the ``smaller" SE - $(Filter_f(employees))$. That's why our identification algorithm does not take into account the obvious worse SEs.

Between Query 1 and Query 2, the SE1, which not only involves projections and filterings on tables but also joining, may be a better sharing option in comparison to SE2 and SE3. Sharing SE1 may not only save I/O operations and computing cost but also reduce significantly the communication cost of joining. ``Greater" SEs are more favored to share in general, however, what if this \emph{Join} operator produces a large amount of data that is too expensive to \emph{cache} under a limited amount of memory? Data caching in memory can be a costly operation, especially in distributed environment. Our study shows that blindly caching data may lead to higher costs of job execution. One next problem we may have to face is to select multiple CEs that will later become \emph{cache plans} such that the job's performance is maximized while the memory consumption condition is satisfied.

Hence, the problem we attempt to solve in this paper can be described as follows: Given a batch of queries submitted, transform it into a more efficient batch such that the new batch results in shorter aggregate job running time, taking into account the memory constraint and extra computations. Particularly in this work, we develop the solution of automatically determining the best cache plans to achieve work sharing for Spark SQL queries. It is the purpose of following sections to first demonstrate and then evaluate our unified solution for an in-memory cached-based work sharing for large-scale distributed systems.